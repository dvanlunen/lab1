---
title: "Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Lab 1"
author: "Daniel Van Lunen,Aaron Olson"
output: 
  pdf_document:
  toc: true
  number_sections: true
fontsize: 11pt
geometry: margin=1in
---

```{r, include=F}
library(ggplot2)
library(dplyr)
library(mcprofile)
library(car)
library(Hmisc)
library(gridExtra)
```

# Introduction
A post-disaster investigation revealed the Challenger explosion was caused by O.ring failures. Before the launch, there was debate about whether to launch due to low temperatures. In this lab, we model the probability of O.ring failure using data from 23 launch tests to investigate the causes of failure and whether a launch at the 31 degree temperature was especially risky. Our findings indicate temperature has a significant impact on O.ring failure, while pressure does not. Our final logistic regression model is

$$
log(\frac{P(O.ring \ Failure)}{P(O.ring \ doesn't \ fail)}) =  5.08498-0.11560\times Temp 
$$
This yields an estimated 81.2% chance of O.ring failure with 90% bootstrapped confidence between 12.5% and 99.3% at the 31 temperature on the day of Challenger loss. This risk could have been avoided by waiting for a warmer day where we predict a much lower chance of failure. For example, at 72 degrees we predict the chance of failure between 1.0% and 6.9% degrees with 90% confidence


# EDA
```{r warning=F}
d <- read.csv('challenger.csv')
d %>% glimpse()
summary(d)
describe(d)
d$Pressure.factor <- factor(d$Pressure,levels = c(50,100,200))
d$O.ring_binary <- factor(ifelse(d$O.ring>0,1,0))
d$O.ring_factor <- factor(d$O.ring, levels = c(0,1,2))
d$Pressure.binary <- factor(ifelse(d$Pressure < 200, 0, 1))
```
The dataset has 5 variables:

1. Flight appears to be an index for each flight and ranges from 1 to 23. 

2. Temp is the Farenheit temperature on the day of the launch

3. Pressure is the psi level pressure used. It increases sequentially with the flight number as the paper mentioned they increased the pressure from 50 to 100 then 200 in their tests.

4. O.ring represents the number of O.ring failures (ranges between 0 and 2)

5. Number is always 6. This is the total number of O.rings.

There are no missing, top-coded, or bottom-coded values.

```{r}
p1 <- ggplot(d, aes(Temp, fill = factor(O.ring), colour = factor(O.ring))) +
 geom_density(alpha=0.2) + geom_histogram(aes(y = ..density..), binwidth = 3, fill="#0072B2", colour="black", alpha = 0.1) + 
  ggtitle("Distribution of Temperature\nfor Flights with and\n without Failures") +
    theme(legend.position="bottom")
p2 <- ggplot(d, aes(Temp, O.ring)) +
 geom_point() +
  ggtitle("Numbers of Failures (O.ring) vs.\n Temperature") + theme(legend.position = "bottom")
grid.arrange(p1,p2,ncol=2)
```
Temperature is skewed to the left with fewer observations for the colder temperatures we are most interested in. For launches with failures the temperature tends to be lower. There also appears to be one temperature that is far from the normal trend: 75 degrees with 2 failures. Given this seems completely possible due to randomness, we leave this point in the data. There is a sharp decline at approximately 65 degrees to nearly no failures above that temperature.

```{r}
# how does the proportion of failures change with Pressure
d %>% 
  group_by(Pressure) %>% 
  summarise(pfailures=mean(O.ring/6))
# how does temperature change with pressure
ggplot(d, aes(Pressure.factor, Temp)) +
  geom_boxplot() + 
  geom_jitter(height = 0, colour=factor(d$O.ring+1)) +
  ggtitle(paste0("Distribution of Temp for each Pressure by Launch\n",
                 "Black Dots have no failures\n",
                 "Red Dots have 1 failure\n",
                 "Green Dots have 2 failures"))

```

Given only 3 values of pressure and only 2 data points at the value of 100, it is harder to judge the impact of pressure on O.ring failure. There are a higher proportion of failures at low pressures and high pressures so we might think this could be caused by failure to seal at low pressures and blow through at high pressures. However, it is possible it is also just explained by the temperature because the temperature for tests conducted at high and low pressures had a lower mean value. Thus, we will see if pressure still has an impact after accounting for temperature.

```{r}
ggplot(d, aes(Flight, Temp, colour = factor(O.ring))) +
 geom_point(alpha=0.8) + 
  ggtitle("Temperature by Flight Number (sequential with time)") +
    theme(legend.position="bottom")
```

While we do not have the data to determine whether or not any changes (material and or process) occurred over the course of the 23 flights, we have visualized the temperature and o-ring failure by flight number (sequential with time). Nothing stands out principally to give evidence that there was structural or otherwise bad batches of O.rings. From flight 9-14, 4 out of 6 flights had at least one o-ring failure, however the mean temperature for these flights is also lower than the mean of the entire flight dataset, and could be explained by temperature. 

Pressure did change over time (correlation - 0.84 between flight number and pressure variables). We have shown plots relating temperature, pressure and O.ring failures previously and will explore the effect (if any) pressure has on predicting O.ring failure. 



# Modeling

## 4a Assumptions Underlying Logistic Model

It is necessary to assume that each O.ring is independent in the binomial model because if the probability that one O.ring fails is dependent on whether another O.ring fails, then the total number of failures will no longer follow a binomial distribution. The assumption is necessary so the authors can use each of the 6 O.rings in each of the 23 launches as a distinct datapoint in the logisitic regression. The problem is that if one O.ring fails, it likely increases the chance that additional O.rings on the same launch will fail because the conditions were "bad" enough to make one fail so they could make others fail as well.

## 4b Estimate Model

```{r}
m4b <- glm(formula = O.ring/Number ~ Temp + Pressure, data = d,
           family = binomial(link = "logit"), weights = Number)
summary(m4b)
```

We also estimate the model with a Pressure squared term because our EDA gives us the hypothesis that potentially middle-ground pressures are ok, but high and low are bad. 

```{r}
m4b_psquared <- glm(formula = O.ring/Number ~ Temp + Pressure + I(Pressure^2), 
                    data = d, family = binomial(link = "logit"), weights = Number)
summary(m4b_psquared)
```
And a model with a pressure and temperature interaction because it is possible that higher temperatures are worse at higher pressures because the rings are more malleable and prone to blow through.
```{r}
m4b_pt <- glm(formula = O.ring/Number ~ Temp + Pressure + Pressure:Temp, 
              data = d, family = binomial(link = "logit"), weights = Number)
summary(m4b_pt)
```
While pressure is commonly a continuous variable, in the context of this dataset, pressure  defines the pressure at which testing was conducted. As there are only three distinct values for the pressure variable, we also inspected a model including pressure as a factor level, as well as an interaction term between factor(pressure) and temperature.

```{r}
m4b_factorp <- glm(formula = O.ring/Number ~ Temp + Pressure.factor, 
              data = d, family = binomial(link = "logit"), weights = Number)
summary(m4b_factorp)

m4b_factorpt <- glm(formula = O.ring_binary ~ Temp + Pressure.factor + Pressure.factor:Temp, 
              data = d, family = binomial(link = "logit"), weights = Number)
summary(m4b_factorpt)
```
For both models which include pressure (as a continuous or categorical variable), Wald statistics show with a strong likelihood, that we can reject that pressure or a pressure/temperature interaction term are statistically significant in the model. 

Therefore we estimate a model with just temperature for comparison.
```{r}
m5a <- glm(formula = O.ring/Number ~ Temp, data = d, 
           family = binomial(link = "logit"), weights = Number)
summary(m5a)
```

## 4c LRTs for variable importance
```{r}
# is temp or pressure signficant after accounting for the other?
Anova(m4b)
# is pressure and pressure^2 signficant after accounting for temp?
anova(m5a,m4b_psquared,test = "LR") 
# is there a significant interaction between temp and pressure
Anova(m4b_pt) 
```

The leave-one-out LR tests indicate that Temperature has a significant impact on the chance of an O.ring failure. This is also consitent with the Wald importance metrics from the model summary previously printed in part 4b. We see that the p-statistic for temperature is 0.0228, using a type one error rate of 0.05 we accept that temperature has a significant impact on predicting the likelihood of o.ring failure. 

On the other hand, after accounting for temperature, Pressure does not have a significant impact. Even when using a higher-order or interaction term, pressure does not have a significant impact after accounting for temperature.

## 4d Removing Pressure and Implications

The authors chose to remove pressure because they found it did not significantly reduce the residual deviance as the LRT above shows.\footnote{they also performed bootstraped confidence intervals on the number of incidents holding pressure constant at 50/200 for different temperatures and found they overlapped greatly.} Removing this variable could potentially bias the coefficient on temperature. Temperature and pressure have a weak positive correlation and temperature has a negative correlation with failures. If pressure also has a negative impact on failures and is left out of the model, part of its impact will be absorbed as bias in the coefficient on temperature, making it seem like temperature is more important than it actually may be in causing a failure. 

## 5a Estimate Model

```{r}
m5a <- glm(formula = O.ring/Number ~ Temp, data = d,
           family = binomial(link = "logit"), weights = Number)
summary(m5a)
```

##  5b Pihat vs Temp and Expected failures vs Temp
Let's see how well the single covariate model lines up with the data by showing (1) the estimated probability of a failure vs. temperature and (2) the expected number of failures vs temperature in charts.
```{r}
alpha <- .05
predict.data <- data.frame(Temp=seq(31,81,by=1))
linear.pred <- predict(object = m5a,newdata = predict.data, type='link', se=T)
predict.data <- predict.data %>% 
  mutate(
    z=linear.pred$fit,
    pi=exp(z)/(1+exp(z)),
    CI.upper.z= z + qnorm(1-alpha/2)*linear.pred$se.fit,
    CI.lower.z= z + qnorm(alpha/2)*linear.pred$se.fit,
    CI.upper.pi = exp(CI.upper.z)/(1+exp(CI.upper.z)),
    CI.lower.pi = exp(CI.lower.z)/(1+exp(CI.lower.z))
  )
predict.data %>% 
  ggplot(aes(Temp)) + 
  geom_ribbon(aes(ymin=CI.lower.pi,ymax=CI.upper.pi), fill="grey70") +
  geom_line(aes(y=pi)) +
  ggtitle(paste0("(1) Estimated Probability of Failure vs Temp\n",
                 "with 95% Wald Confidence Bands"))
```

```{r}
data.frame(Temp <- seq(31,81,by=1)) %>% 
  mutate(
    z=m5a$coefficients["(Intercept)"] + m5a$coefficients["Temp"]*Temp,
    predicted_failures=6*exp(z)/(1+exp(z))
    ) %>% 
  ggplot(aes(Temp,predicted_failures)) + geom_line() + 
  ggtitle("(2) Predicted Failures vs Temp with Overlaid True Points") +
  geom_point(data=d %>% mutate(predicted_failures=O.ring))
```

## 5c 95% Wald confidence bands
The confidence bands on $pi$ are wider for lower temperatures because we do not have any data points at those low data points. The more we have to extrapolate, the less confident we will be (there is larger variance in predictions with fewer training points).

## 5d Confidence interval at 31 degrees and assumptions
```{r}
# Wald
predict.data %>% filter(Temp==31) %>% select(Temp,pi,CI.lower.pi,CI.upper.pi)

# LR profile
z <- mcprofile(object = m5a, CM = matrix(data = c(1, 31), nrow=1, ncol=2))
z.CI <- confint(object = z,level = .95)
(pi.CI.LR.profile <- exp(z.CI$confint)/(1+exp(z.CI$confint)))

```
The estimated probability of a failure at this temperature is about 81.8%. The LR profile confidence interval for the probability of failure of a given O.ring is between approximately 14.2% and 99.1%. This is in rough agreement with the Wald confidence interval. The assumptions are
1. All O.ring failure probabilities are identical given temperature.
2. O.ring failures are independent of each other (i.e we trained on a random sample of O.ring tests). This is questionable because O.rings are clustered by launch (6 always happen in the same conditions).
3. The log-odds of an O.ring failure is linearly related to the Temperature.
4. After holding temperature constant there are no other variables that affect the probability of failure. This exogeneity assumption seems tenuous, but we do not have any other variables besides pressure to place in the regression.

From the model above that only had temperature as the explanatory variable, we found the residual deviance to be 18.09. 

5. The sample is large enough that the estimated linear combination of coefficients is normal (for Wald) or $-2log(LR) \sim \chi^2$ (for the profile interval).
6. The model has converged to the true values of the parameters.
7. There is no perfect colinearity of the independent variables (here this is met because we only have Temp as an indpendent variable).

## 5e Parametric Bootstrap
```{r warning=F}
generate_pihats <- function(logisticmodel=m5a, data=d,
                            temps=data.frame(Temp=c(31,72))){
  # 1. Random sample of temperatures size nrow(d) from d with replacement
  bootstrap_df <- data.frame(Temp=sample(data$Temp, size = nrow(data), replace = T),
                             Number=6)
  # 2. Use model to estimate pi at each of those temperatures
  bootstrap_df$model.pi.hat <- predict(object = logisticmodel, 
                                       newdata = bootstrap_df, type='response')
  # 3. Using those estimated pis, make random outcomes from binom(6, pi)
  bootstrap_df$O.ring <- rbinom(nrow(data), 6, bootstrap_df$model.pi.hat)
  # 4. Estimate a new logistic regression with those random outcomes and temperatures
  bootstrap_m <- glm(formula = O.ring/Number ~ Temp, data = bootstrap_df,
                     family = binomial(link = "logit"), weights = Number)
  # 5. Compute estimated pis at 31 and 72 degrees
  cbind(temps,pihat=predict(object = bootstrap_m, newdata = temps, type='response'))
  
}
# 6. Replicate many times
set.seed(42)
bootstrapped_pis <- do.call("rbind", replicate(10000, generate_pihats(),simplify = F))
# 7. Take 5th and 95th percentile of estimates for 31,72 as confidence interval
bootstrapped_pis %>% 
  group_by(Temp) %>% 
  summarise(CI.lower=quantile(pihat, probs=.05),
            CI.upper=quantile(pihat, probs=.95))

# density plots to visualize
bootstrapped_pis %>%
  ggplot(aes(x=pihat,fill=factor(Temp))) +
  geom_density(alpha=.3) +
  ggtitle('Parametric Bootstrap Pihats at 31 and 72 degrees')

```
Our parametric bootstraps indicate that we estimate the probability of failure for a given O.ring to bet between about 12.5% and 99.3% at 31 degrees and 1.0% and 6.9% at 72 degrees. Again the interval is much wider for lower temperatures where we have fewer data points. 

Note in our bootstrap sometimes we get complete separation, but this just makes our estimates 0 or 1 for the predicted probabilities which drop out after using the confidence interval.

## 5f is a quadratic term necessary?
```{r}
# empirical proportion of failures for buckets of temperatures
d$tempbuckets <- cut(d$Temp, breaks=c(-Inf,46,53,60,67,74,Inf))
d %>% group_by(tempbuckets) %>% 
  summarise(empiricallogodds=log(mean(O.ring/6)/(1-mean(O.ring/6))),
            numtotal=sum(Number)) %>%
  ggplot(aes(x=tempbuckets,y=empiricallogodds,size=numtotal)) +
  geom_point()
```

Plotting the empircal log odds for different buckets reveals that above a certain temperature, failures seem quite rare. There does not seem to be enough data to confirm if the relationship is quadratic from a plot like this. Let's instead use a more general case LRT to see if the term on the squared temperature is significant.

```{r}
# estimate new model with quadratic term
m5f <- glm(formula = O.ring/Number ~ Temp + I(Temp^2), data = d, family = binomial(link = "logit"), weights = Number)
Anova(m5f, test.statistic = "LR")
```

From the profile likelihood ratio results, we have a P-statistic of 0.482 for the quadratic temperature term. This interprets as we fail to reject the notion that temperature squared has a significant impact on the prediction of O.ring failure likelihood. 

# Interpret final model
After seeing that pressure, a higher order pressure term, a temperature-pressure interaction term, and a higher order temperature term all are not close to signficant, we arrive at the same final model as the authors of the paper.
```{r}
summary(m5a)
exp(-5*m5a$coefficients["Temp"]) # OR multiplier for 5 degree decrease
```

Here for every 5 degree decrease in temperature, the odds of a failure for a given O.ring increase by 78.2%. The estimated probability of failure can be seen above in the chart for 5b. Noteably, the higher temperatures have a lower probability of failure even when accounting for the confidence bands. 

# What about a Linear Probability Model?
## Estimate the model
```{r}
failures <- c()
temps <- c()
for (i in 1:nrow(d)){
  failures <- c(failures,rep(1,d$O.ring[i]),rep(0,6-d$O.ring[i]))
  temps <- c(temps,rep(d$Temp[i],6))
}
d_LPM <- data.frame(failures=failures,Temp=temps)
LPM <- lm(failures~Temp,data = d_LPM)
summary(LPM)
```
This linear probability model estimates that the probability of an O.ring failure decreases linearly with temperature. Every 5 degree decrease in temperature would increase the estimated probability of failure by 0.79% (absolute not relative change). Let's also check the linear model's assumptions.

## Assumptions
### Linear model, Random Sample, No Perfect Collinearity
The first assumption is that the outcome is a linear function of the predictors. Though we are not placing any conditions yet on the error term, so this assumption is met, we can already see that a linear model may not be right because under this model, at temperatures above about 77.8 degrees, we predict a negative probability of failure, which does not make sense (especially given our data has data points here).

The linear model assumes a random sample, which is likely incorrect here for the same reasons mentioned in the logistic regression that outcomes on a given launch are likely correlated.

The no perfect collinearity assumption is met.

### Zero conditional mean
```{r}
ggplot(LPM, aes(.fitted, .resid))+geom_point() +
    stat_smooth(method="loess")+geom_hline(yintercept=0, col="red", linetype="dashed")+
    xlab("Fitted values")+ylab("Residuals") +
    ggtitle("Residual vs Fitted Plot")+theme_bw()
```
The residuals vs fitted value plot has confidence bands that contain zero throughout. However, there is an obvious, linearly decreasing pattern for two sets of points in the plot indicating there is some association between the error term and the fitted values. Therefore the temperature coefficient is likely biased. 

### Homoskedasticity
To get standard errors on the coefficients, we need a constant error variance.
```{r}
ggplot(LPM, aes(.fitted, sqrt(abs(.stdresid))))+geom_point(na.rm=TRUE)+
    stat_smooth(method="loess", na.rm = TRUE)+xlab("Fitted Value")+
    ylab(expression(sqrt("|Standardized residuals|")))+
    ggtitle("Scale-Location")+theme_bw()
```
The Scale-Location plot reveals that the variance of the error term is clearly not constant: there is an upward trend in the variance of the residuals as the fitted values increase. Though we could use heteroskedastic-robust errors, the failure of the other assumptions makes the LPM a rough choice.

### Normally Distributed Error
To conduct hypothesis t-tests for the LPM, we need normally distributed errors.
```{r}
df <- data.frame(residuals=LPM$residuals)
ggplot(df, aes(sample=residuals)) +
    stat_qq() + stat_qq_line() +
    xlab("Theoretical Quantiles") + ylab("Standardized Residuals") +
    ggtitle("Normal Q-Q")
```
The residuals deviate significantly from the normal distribution.

## Linear or Logistic?
A logistic model is much better because it keeps the estimated probabilities within $(0,1)$ and doesn't require as many assumptions that are clearly failing with the linear model.

## Original analysis prior to launch vs Dalal
When preparing for launch the authors detailed that the analysis conducted in order to determine whether the forecast low tempearture would affect flight success probability only focused on flights that had failed O.rings rather than including successful flights. In the interest of analysis, we have limited the dataset below to only flights that had failures and conduct fitting of a logistic regression model. 

```{r}
df_fail<- d[c(2,9,10,11,14,21,23),]
m_original <- glm(formula = O.ring/Number ~ Temp, data = df_fail,
           family = binomial(link = "logit"), weights = Number)
summary(m_original)

m_original_temp <- glm(formula = O.ring/Number ~ Temp + I(Temp^2), data = df_fail,
           family = binomial(link = "logit"), weights = Number)
summary(m_original_temp)
```
```{r}
Anova(m_original)
anova(m_original, m_original_temp, test = 'Chisq')
```

From the model, Wald and LR test statistics, we can see that none of the parameters for estimation were statistically significant. This is likely due to the limited datset (7 flights), as well as the absence of successful flights from the dataset. 

```{r}
# Wald - Using Temp^2 term
predict.data <- data.frame(Temp = 31)
linear.pred = predict(object = m_original_temp, newdata = predict.data, type = "link", se = TRUE)
pi.hat = exp(linear.pred$fit)/(1+exp(linear.pred$fit))
CI.lin.pred = linear.pred$fit + qnorm(p = c(alpha/2, 1-alpha/2))*linear.pred$se
CI.pi = exp(CI.lin.pred)/(1+exp(CI.lin.pred))
round(data.frame(pi.hat, lower=CI.pi[1], upper=CI.pi[1]),4)


# LR profile
z <- mcprofile(object = m_original_temp, CM = matrix(data = c(1, 31, 31^2), nrow=1, ncol=3))
z.CI <- confint(object = z,level = .95)
(pi.CI.LR.profile <- exp(z.CI$confint)/(1+exp(z.CI$confint)))

# Wald - Without Temp^2 term
predict.data <- data.frame(Temp = 31)
linear.pred = predict(object = m_original, newdata = predict.data, type = "link", se = TRUE)
pi.hat = exp(linear.pred$fit)/(1+exp(linear.pred$fit))
CI.lin.pred = linear.pred$fit + qnorm(p = c(alpha/2, 1-alpha/2))*linear.pred$se
CI.pi = exp(CI.lin.pred)/(1+exp(CI.lin.pred))
round(data.frame(pi.hat, lower=CI.pi[1], upper=CI.pi[1]),4)


# LR profile
z <- mcprofile(object = m_original, CM = matrix(data = c(1, 31), nrow=1, ncol=2))
z.CI <- confint(object = z,level = .95)
(pi.CI.LR.profile <- exp(z.CI$confint)/(1+exp(z.CI$confint)))
```

Predicting the probability of failure at a Temperature of 31 degrees, we investigated both models with and without a quadratic temprature term. In all cases, we can see that the range in likely number of O.ring failures is nearly between the limits of a binomial distribution (0,1) -> furthermore the dataset contained no flights where teh expected value($\pi > 2/6$). This confidence interval shows that the dataset and/or determinations made from the logistic model likely aren't conclusive or valid. 

Using the model $logit(\pi) = \beta_0 + \beta_1 * Temperature$ there is an expected proportion of o-rings that fail as 0.21 or between 1 and 2 O.ring failures. For the model $logit(\pi) = beta_0 + \beta_1*Temperature + \beta_2*Temperature^2$ the predicted value is nearly all O.rings failing during flight (a statistic that wasn't observed in the dataset).

From this investigation utilizing a dataset comprised of only O.ring failures, we can determine that there are no meaningful results provided by either model using both confidence intervals and statisticall significance of the explanatory variable coefficients. 

# Conclusion
Decision makers at NASA would have done well to have statistically modelled the probability of O.ring failure before the Challenger launch. In this lab, we found that there was a strong relationship between O.ring failure and temperature. Our find model was 
$$
log(\frac{P(O.ring \ Failure)}{P(O.ring \ doesn't \ fail)}) =  5.08498-0.11560\times Temp 
$$
This model indicates that the Challenger launch should have been delayed due to the high estimated probability of failure of the O.rings (between 12.5% and 99.3% chance of failure at the 31 temperature on chosen launch day with 90% confidence). This high estimated probability of failure could have convinced decision makers to wait until the temperature was higher. For example, until the temperature was 72 degrees where the predicted chance of failure is between 1.0% and 6.9% with 90% confidence. This could have potentially averted disaster.




